{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "MLP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "import tensorflow as tf\r\n",
        "from geneticalgorithm import geneticalgorithm as ga\r\n",
        "import datetime\r\n",
        "import math\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, LeakyReLU, BatchNormalization, Flatten\r\n",
        "from keras.callbacks import TensorBoard\r\n",
        "from keras.optimizers import Adam\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "import tensorflow as tf"
      ],
      "outputs": [],
      "metadata": {
        "id": "RRAbfAS7KsVr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "call_df = pd.read_csv('')\r\n",
        "History = pd.read_csv('')\r\n",
        "risk_free_asset = pd.read_csv('')"
      ],
      "outputs": [],
      "metadata": {
        "id": "JMkkYPV3KwsV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Format and split data before training"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\r\n",
        "# Add risk-free asset as feature to df on date\r\n",
        "n_timesteps = 1\r\n",
        "\r\n",
        "padded = np.insert(risk_free_asset.Rate.values, 0, np.array([np.nan] * n_timesteps))\r\n",
        "rolled = np.column_stack([np.roll(padded, i) for i in range(n_timesteps)])\r\n",
        "rolled = rolled[~np.isnan(rolled).any(axis=1)]\r\n",
        "rolled = np.column_stack((risk_free_asset.Date.values[n_timesteps - 1:], rolled))\r\n",
        "price_history = pd.DataFrame(data=rolled)\r\n",
        "joined = call_df.join(price_history.set_index(0), on='Date')\r\n",
        "\r\n",
        "joined['r'] = joined[1]\r\n",
        "call_df = joined.drop(columns=[1,'LTP', 'Risk_free_rate'],axis=1)\r\n",
        "\r\n",
        "\r\n",
        "#Creates the stock dynamics for the n_timesteps back.\r\n",
        "\r\n",
        "underlying=History\r\n",
        "n_timesteps = 30\r\n",
        "padded = np.insert(underlying.Close.values, 0, np.array([np.nan] * n_timesteps))\r\n",
        "rolled = np.column_stack([np.roll(padded, i) for i in range(n_timesteps)])\r\n",
        "rolled = rolled[~np.isnan(rolled).any(axis=1)]\r\n",
        "rolled = np.column_stack((underlying.Date.values[n_timesteps - 1:], rolled))\r\n",
        "price_history = pd.DataFrame(data=rolled)\r\n",
        "joined = call_df.join(price_history.set_index(0), on='Date')\r\n",
        "call_df=joined\r\n",
        "call_df = call_df.drop(columns=['Date','Expiry'])\r\n",
        "call_df = call_df.dropna()\r\n",
        "\r\n",
        "features = 4"
      ],
      "outputs": [],
      "metadata": {
        "id": "VLm782nZA3qR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "call_df=call_df[['Strike Price','Close','nDiff','r','Underlying Value']]"
      ],
      "outputs": [],
      "metadata": {
        "id": "yncuCXcqAvAL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "call_X_train, call_X_test, call_y_train, call_y_test = train_test_split(call_df.drop(columns=['Close'],axis=1).values,\r\n",
        "                                                                        (call_df['Close']).values,\r\n",
        "                                                                        #shuffle=False,\r\n",
        "                                                                         random_state=42,\r\n",
        "                                                                  test_size=0.01)\r\n",
        "call_X_test= np.array(call_X_test, dtype=np.float64)\r\n",
        "call_y_test= np.array(call_y_test, dtype=np.float64)\r\n",
        "call_X_train=np.asarray(call_X_train).astype(np.float64)"
      ],
      "outputs": [],
      "metadata": {
        "id": "x-JaIuoEMkeM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "varbound=np.array([[1,800],[1,800],[1,12]])\r\n",
        "algorithm_param = {'max_num_iteration': 100,\\\r\n",
        "        'population_size':15,\\\r\n",
        "        'mutation_probability':0.1,\\\r\n",
        "        'elit_ratio': 0.01,\\\r\n",
        "        'crossover_probability': 0.5,\\\r\n",
        "        'parents_portion': 0.3,\\\r\n",
        "        'crossover_type':'uniform',\\\r\n",
        "        'max_iteration_without_improv':7}"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calibrate"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def MLP_GA():\r\n",
        "    def f(x):\r\n",
        "            n_units,n_units_hidden,layers = x\r\n",
        "            model = Sequential()\r\n",
        "            model.add(Dense(n_units, input_dim=call_X_train.shape[1]))\r\n",
        "            model.add(LeakyReLU())\r\n",
        "\r\n",
        "            for _ in range(int(layers) - 1):\r\n",
        "                model.add(Dense(n_units_hidden))\r\n",
        "                model.add(BatchNormalization())\r\n",
        "                model.add(LeakyReLU())\r\n",
        "\r\n",
        "            model.add(Dense(1, activation='relu'))\r\n",
        "\r\n",
        "            model.compile(loss='mse', optimizer=Adam())\r\n",
        "\r\n",
        "            model.fit(call_X_train, call_y_train, \r\n",
        "                    batch_size=250, epochs=200, \r\n",
        "                    validation_split = 0.01,\r\n",
        "                    callbacks=[TensorBoard()],\r\n",
        "                    verbose=0)\r\n",
        "            \r\n",
        "            return np.sqrt(np.mean(np.square(call_y_test-model.predict(call_X_test, batch_size=250).reshape(call_y_test.shape[0]))))\r\n",
        "        \r\n",
        "    model=ga(function=f,\\\r\n",
        "            dimension=3,\\\r\n",
        "            variable_type='int',\\\r\n",
        "            variable_boundaries=varbound,\\\r\n",
        "            function_timeout=4000,\r\n",
        "            algorithm_parameters=algorithm_param,\r\n",
        "         convergence_curve=True,\r\n",
        "         progress_bar=True)\r\n",
        "\r\n",
        "    model.run()\r\n",
        "    return model.best_variable"
      ],
      "outputs": [],
      "metadata": {
        "id": "wXZtel8DKhaX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "MLP_params=MLP_GA()"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "SG0cRcfgMPsd",
        "outputId": "0dea3be1-0bfa-4aa8-dff8-9af686e7a1be"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Option traning"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "n_units , n_units_hidden, layers = MLP_params"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Read all data again to disregard all changes made previously\r\n",
        "\r\n",
        "call_df = pd.read_csv('')\r\n",
        "History = pd.read_csv('')\r\n",
        "risk_free_asset = pd.read_csv('')"
      ],
      "outputs": [],
      "metadata": {
        "id": "zYwqzonFMlUU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "n_timesteps = 1\r\n",
        "\r\n",
        "padded = np.insert(risk_free_asset.Rate.values, 0, np.array([np.nan] * n_timesteps))\r\n",
        "rolled = np.column_stack([np.roll(padded, i) for i in range(n_timesteps)])\r\n",
        "rolled = rolled[~np.isnan(rolled).any(axis=1)]\r\n",
        "rolled = np.column_stack((risk_free_asset.Date.values[n_timesteps - 1:], rolled))\r\n",
        "price_history = pd.DataFrame(data=rolled)\r\n",
        "joined =call_df.join(price_history.set_index(0), on='Date')\r\n",
        "\r\n",
        "joined['r']=joined[1]\r\n",
        "call_df=joined.drop(columns=[1,'LTP', 'Risk_free_rate'],axis=1)\r\n",
        "\r\n",
        "\r\n",
        "#Creates the stock dynamics for the n_timesteps back.\r\n",
        "underlying=History\r\n",
        "n_timesteps = 30\r\n",
        "padded = np.insert(underlying.Close.values, 0, np.array([np.nan] * n_timesteps))\r\n",
        "rolled = np.column_stack([np.roll(padded, i) for i in range(n_timesteps)])\r\n",
        "rolled = rolled[~np.isnan(rolled).any(axis=1)]\r\n",
        "rolled = np.column_stack((underlying.Date.values[n_timesteps - 1:], rolled))\r\n",
        "price_history = pd.DataFrame(data=rolled)\r\n",
        "joined = call_df.join(price_history.set_index(0), on='Date')\r\n",
        "call_df=joined\r\n",
        "call_df = call_df.drop(columns=['Date','Expiry'])\r\n",
        "call_df = call_df.dropna()"
      ],
      "outputs": [],
      "metadata": {
        "id": "jwSaCxq6Un7Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\r\n",
        "call_X_train, call_X_test, call_y_train, call_y_test = train_test_split(call_df.drop(columns=['Close'],axis=1).values,\r\n",
        "                                                                        (call_df['Close']).values,\r\n",
        "                                                                        #shuffle=False,\r\n",
        "                                                                         random_state=42,\r\n",
        "                                                                  test_size=0.01)\r\n",
        "call_X_test= np.array(call_X_test, dtype=np.float64)\r\n",
        "call_y_test= np.array(call_y_test, dtype=np.float64)\r\n",
        "call_X_train=np.asarray(call_X_train).astype(np.float64)\r\n",
        "\r\n",
        "call_y_train=np.asarray(call_y_train).astype(np.float64)\r\n",
        "\r\n",
        "n_timesteps = 30\r\n",
        "features = 4"
      ],
      "outputs": [],
      "metadata": {
        "id": "YWd1W6coUqEj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "model = Sequential()\r\n",
        "model.add(Dense(n_units, input_dim=call_X_train.shape[1]))\r\n",
        "model.add(LeakyReLU())\r\n",
        "\r\n",
        "for _ in range(layers - 1):\r\n",
        "    model.add(Dense(n_units_hidden))\r\n",
        "    model.add(LeakyReLU())\r\n",
        "\r\n",
        "model.add(Dense(1, activation='relu'))\r\n",
        "\r\n",
        "model.compile(loss='mse', optimizer=Adam(learning_rate=1e-4))"
      ],
      "outputs": [],
      "metadata": {
        "id": "W-pPmMUZ9_iI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "history = model.fit(call_X_train, call_y_train, \r\n",
        "                    batch_size=4900, epochs=290, \r\n",
        "                    validation_split = 0.01,\r\n",
        "                    callbacks=[tensorboard_callback],\r\n",
        "                    verbose=1)\r\n",
        "model.save('call-mlp.h5')"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqYazZsJAwvO",
        "outputId": "5d6bc6ea-aaa8-4a04-9886-e56572b89feb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metrics"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from .utilties import utilties"
      ],
      "outputs": [],
      "metadata": {
        "id": "0H8CE5UOAzcL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "line1 = utilties.error_metrics(call_y_test, model.predict(call_X_test, batch_size=4900).reshape(call_y_test.shape[0]))"
      ],
      "outputs": [],
      "metadata": {
        "id": "Jt_Gw1jX_Rv8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print('MSE: {:.2f} & RMSE: {:.2f} & BIAS: {:.2f}% & AAPE: {:.2f}% & MAPE: {:.2f}% & PE5: {:.2f}\\% & PE10: {:.2f}% & PE20: {:.2f}% '.format(*line1))"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNxZpcJn_Udx",
        "outputId": "e3b7707f-f376-4583-c7d3-37db8c63535d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Returns"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Create a dataframe with only the Close Stock Price Column\r\n",
        "data_target = History.filter(['Close'])\r\n",
        "\r\n",
        "# Convert the dataframe to a numpy array to train the LSTM model\r\n",
        "target = data_target.values\r\n",
        "\r\n",
        "# Splitting the dataset into training and test\r\n",
        "# Target Variable: Close stock price value\r\n",
        "\r\n",
        "training_data_len = math.ceil(len(target)* 0.75) # training set has 75% of the data\r\n",
        "training_data_len\r\n",
        "\r\n",
        "# Normalizing data before model fitting using MinMaxScaler\r\n",
        "# Feature Scaling\r\n",
        "sc = MinMaxScaler(feature_range=(0,1))\r\n",
        "training_scaled_data = sc.fit_transform(target)"
      ],
      "outputs": [],
      "metadata": {
        "id": "IMMM0dd8v198"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Create a training dataset containing the last 30-day closing price values we want to use to estimate the 31st closing price value.\r\n",
        "train_data = training_scaled_data[0:training_data_len  , : ]\r\n",
        "\r\n",
        "X_train = []\r\n",
        "y_train = []\r\n",
        "for i in range(30, len(train_data)):\r\n",
        "    X_train.append(train_data[i-30:i, 0])\r\n",
        "    y_train.append(train_data[i, 0])\r\n",
        "\r\n",
        "X_train, y_train = np.array(X_train), np.array(y_train) # converting into numpy sequences to train the LSTM model\r\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\r\n",
        "print('Number of rows and columns: ', X_train.shape)"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnieQ5GywFhd",
        "outputId": "bfbb0953-5f09-409a-8ad5-527943db0f7b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "with tf.device('/Gpu:0'): \r\n",
        "  model = Sequential()\r\n",
        "  model.add(Dense(n_units, input_shape= (X_train.shape[1], 1)))\r\n",
        "  model.add(LeakyReLU())\r\n",
        "  model.add(Flatten()) #Makes the output flat.\r\n",
        "  # Adding the output layer\r\n",
        "  model.add(Dense(units = 1))\r\n",
        "  # Compiling the RNN\r\n",
        "  model.compile(optimizer = 'adam', loss = 'mean_squared_error')\r\n",
        "  # Fitting the RNN to the Training set\r\n",
        "  model.fit(X_train, y_train, epochs = 90, batch_size = 1)"
      ],
      "outputs": [],
      "metadata": {
        "id": "snH984nt0VzE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Getting the predicted stock price\r\n",
        "test_data = training_scaled_data[training_data_len - 30: , : ]\r\n",
        "\r\n",
        "#Create the x_test and y_test data sets\r\n",
        "X_test = []\r\n",
        "y_test =  target[training_data_len : , : ]\r\n",
        "for i in range(30,len(test_data)):\r\n",
        "    X_test.append(test_data[i-30:i,0])\r\n",
        "\r\n",
        "# Convert x_test to a numpy array\r\n",
        "X_test = np.array(X_test)\r\n",
        "\r\n",
        "#Reshape the data into the shape accepted by the LSTM\r\n",
        "X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))\r\n",
        "print('Number of rows and columns: ', X_test.shape)"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z68g80OewM2z",
        "outputId": "05f1e676-dd0a-4832-d381-78a846ed499b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "predicted_stock_price = model.predict(X_test)\r\n",
        "predicted_stock_price = sc.inverse_transform(predicted_stock_price)\r\n",
        "predicted_stock_price_2 = model.predict(X_train)\r\n",
        "predicted_stock_price_2 = sc.inverse_transform(predicted_stock_price)\r\n",
        "valid = data_target[training_data_len:]"
      ],
      "outputs": [],
      "metadata": {
        "id": "kzkbxb-gwPQB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save returns"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "valid.to_csv('MLP-STOCK.csv')"
      ],
      "outputs": [],
      "metadata": {
        "id": "z8eLe-7dwUdO"
      }
    }
  ]
}